import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# Loading the dataset
data = pd.read_csv("/content/Creditcard_data.csv")
print(data.head())
print(data.info())
print(data['Class'].value_counts())  # Check class imbalance

#Applying SMOTE for balancing the dataset
X = data.drop(columns='Class')
y = data['Class']

smote = SMOTE(random_state=42)
X_balanced, y_balanced = smote.fit_resample(X, y)

print("Balanced Class Distribution:")
print(y_balanced.value_counts())


balanced_data = pd.concat([X_balanced, pd.Series(y_balanced, name='Class')], axis=1)

#Generating five different samples
sample1 = balanced_data.sample(n=200, random_state=1)  # Random sampling
sample2 = balanced_data.sample(n=200, random_state=2)  # Random sampling with a different seed
_, sample3 = train_test_split(balanced_data, test_size=0.25, stratify=balanced_data['Class'], random_state=3)  # Stratified sampling
sample4 = balanced_data.head(200)  # First 200 rows as a sample
sample5 = balanced_data.tail(200)  # Last 200 rows as a sample


samples = [sample1, sample2, sample3, sample4, sample5]
for i, sample in enumerate(samples, 1):
    sample.to_csv(f"sample{i}.csv", index=False)

# Defining models to use
models = {
    "Random Forest": RandomForestClassifier(random_state=42),
    "Naive Bayes": GaussianNB(),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric="logloss"),
    "KNN": KNeighborsClassifier(),
    "Logistic Regression": LogisticRegression(max_iter=1000),
}

# Applying models to each sample
for i, sample in enumerate(samples, 1):
    print(f"Results for Sample {i}")
    X = sample.drop(columns='Class')
    y = sample['Class']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )

    if len(y_train.unique()) < 2 or len(y_test.unique()) < 2:
        print(f"Skipping Sample {i}: Not enough class representation in train/test split.")
        continue

    for model_name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        print(f"{model_name}: Accuracy = {accuracy:.2f}")
    print("-" * 55)
